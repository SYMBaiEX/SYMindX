/**
 * Reinforcement Learning Module for SYMindX
 *
 * Implements advanced RL capabilities:
 * - RLHF (Reinforcement Learning from Human Feedback)
 * - Reward modeling and shaping
 * - Multi-agent RL environments
 * - Curriculum learning
 * - Various RL algorithms (DQN, PPO, A3C, etc.)
 */

import { BaseLearningModule } from '../base-learning-module';
import {
  LearningConfig,
  LearningExperience,
  LearningUpdate,
  AdaptationContext,
  AdaptationResult,
  TestData,
  LearningMetrics,
  LearningParadigm,
  RLConfig,
  RLAlgorithm,
  ExplorationStrategy,
  RewardShaping,
  RewardModel,
  HumanFeedback,
  ComparisonFeedback,
  MultiAgentConfig,
  Curriculum,
  CurriculumStage,
  UpdateType,
  ParameterChanges,
  EnvironmentFeedback,
} from '../types';
import { runtimeLogger } from '../../../utils/logger';
import { Experience } from '../../../types/autonomous';

/**
 * Reward model for RLHF
 */
class HumanFeedbackRewardModel implements RewardModel {
  private feedbackHistory: HumanFeedback[] = [];
  private comparisonPairs: ComparisonFeedback[] = [];
  private rewardFunction: Map<string, number> = new Map();
  private uncertaintyEstimates: Map<string, number> = new Map();
  private ensembleSize: number = 5;
  private ensembleModels: Array<Map<string, number>> = [];

  constructor() {
    // Initialize ensemble for uncertainty estimation
    for (let i = 0; i < this.ensembleSize; i++) {
      this.ensembleModels.push(new Map());
    }
  }

  predict(state: any, action: any): number {
    const key = this.getStateActionKey(state, action);

    // Use ensemble average
    let totalReward = 0;
    let validModels = 0;

    for (const model of this.ensembleModels) {
      if (model.has(key)) {
        totalReward += model.get(key)!;
        validModels++;
      }
    }

    if (validModels > 0) {
      return totalReward / validModels;
    }

    // Default reward based on similarity to known states
    return this.predictFromSimilarity(state, action);
  }

  update(feedback: HumanFeedback): void {
    this.feedbackHistory.push(feedback);

    if (feedback.comparison) {
      this.comparisonPairs.push(feedback.comparison);
      this.updateFromComparison(feedback.comparison);
    } else {
      this.updateFromRating(feedback);
    }

    // Update uncertainty estimates
    this.updateUncertainty();

    runtimeLogger.debug(
      `ðŸ“Š Updated reward model with human feedback, ` +
        `total feedback: ${this.feedbackHistory.length}`
    );
  }

  uncertainty(state: any, action: any): number {
    const key = this.getStateActionKey(state, action);

    if (this.uncertaintyEstimates.has(key)) {
      return this.uncertaintyEstimates.get(key)!;
    }

    // Calculate ensemble disagreement
    const predictions: number[] = [];

    for (const model of this.ensembleModels) {
      if (model.has(key)) {
        predictions.push(model.get(key)!);
      }
    }

    if (predictions.length < 2) return 1.0; // Maximum uncertainty

    // Calculate standard deviation
    const mean = predictions.reduce((a, b) => a + b, 0) / predictions.length;
    const variance =
      predictions.reduce((sum, pred) => sum + Math.pow(pred - mean, 2), 0) /
      predictions.length;

    return Math.sqrt(variance);
  }

  private getStateActionKey(state: any, action: any): string {
    return `${JSON.stringify(state)}_${JSON.stringify(action)}`;
  }

  private updateFromRating(feedback: HumanFeedback): void {
    const [state, action] = feedback.stateActionPair;
    const key = this.getStateActionKey(state, action);

    // Update each ensemble model with bootstrapped data
    for (let i = 0; i < this.ensembleModels.length; i++) {
      if (Math.random() < 0.8) {
        // Bootstrap sampling
        const model = this.ensembleModels[i];
        const currentReward = model.get(key) || 0;
        const learningRate = 0.1;

        model.set(
          key,
          currentReward + learningRate * (feedback.rating - currentReward)
        );
      }
    }
  }

  private updateFromComparison(comparison: ComparisonFeedback): void {
    const [preferredState, preferredAction] = comparison.preferred;
    const [rejectedState, rejectedAction] = comparison.rejected;

    const preferredKey = this.getStateActionKey(
      preferredState,
      preferredAction
    );
    const rejectedKey = this.getStateActionKey(rejectedState, rejectedAction);

    const margin = comparison.margin || 0.1;

    // Bradley-Terry model update
    for (const model of this.ensembleModels) {
      if (Math.random() < 0.8) {
        // Bootstrap sampling
        const preferredReward = model.get(preferredKey) || 0;
        const rejectedReward = model.get(rejectedKey) || 0;

        const prob = 1 / (1 + Math.exp(rejectedReward - preferredReward));
        const gradient = margin * (1 - prob);

        model.set(preferredKey, preferredReward + 0.1 * gradient);
        model.set(rejectedKey, rejectedReward - 0.1 * gradient);
      }
    }
  }

  private predictFromSimilarity(state: any, action: any): number {
    // Find similar state-action pairs
    let weightedReward = 0;
    let totalWeight = 0;

    for (const feedback of this.feedbackHistory.slice(-100)) {
      const [fbState, fbAction] = feedback.stateActionPair;
      const similarity =
        this.calculateSimilarity(state, fbState) *
        this.calculateSimilarity(action, fbAction);

      if (similarity > 0.5) {
        weightedReward += similarity * feedback.rating;
        totalWeight += similarity;
      }
    }

    return totalWeight > 0 ? weightedReward / totalWeight : 0;
  }

  private calculateSimilarity(a: any, b: any): number {
    // Simple similarity metric
    if (typeof a === 'number' && typeof b === 'number') {
      return 1 - Math.abs(a - b) / (Math.abs(a) + Math.abs(b) + 1);
    }

    if (Array.isArray(a) && Array.isArray(b)) {
      const minLen = Math.min(a.length, b.length);
      let similarity = 0;

      for (let i = 0; i < minLen; i++) {
        similarity += this.calculateSimilarity(a[i], b[i]);
      }

      return similarity / Math.max(a.length, b.length);
    }

    return a === b ? 1 : 0;
  }

  private updateUncertainty(): void {
    // Update uncertainty estimates based on ensemble disagreement
    const allKeys = new Set<string>();

    for (const model of this.ensembleModels) {
      for (const key of model.keys()) {
        allKeys.add(key);
      }
    }

    for (const key of allKeys) {
      const predictions: number[] = [];

      for (const model of this.ensembleModels) {
        if (model.has(key)) {
          predictions.push(model.get(key)!);
        }
      }

      if (predictions.length >= 2) {
        const mean =
          predictions.reduce((a, b) => a + b, 0) / predictions.length;
        const variance =
          predictions.reduce((sum, pred) => sum + Math.pow(pred - mean, 2), 0) /
          predictions.length;

        this.uncertaintyEstimates.set(key, Math.sqrt(variance));
      }
    }
  }
}

/**
 * Curriculum manager for progressive learning
 */
class CurriculumManager {
  private currentStage: number = 0;
  private stageProgress: Map<string, number> = new Map();
  private performanceHistory: number[] = [];

  constructor(private curriculum: Curriculum) {}

  getCurrentStage(): CurriculumStage {
    return this.curriculum.stages[this.currentStage];
  }

  updateProgress(taskId: string, performance: number): boolean {
    this.stageProgress.set(taskId, performance);
    this.performanceHistory.push(performance);

    // Check if ready to progress
    if (this.shouldProgress()) {
      return this.progressToNextStage();
    }

    return false;
  }

  private shouldProgress(): boolean {
    const stage = this.getCurrentStage();
    const criteria = stage.completionCriteria;

    // Get recent performance
    const window = Math.min(criteria.window, this.performanceHistory.length);
    const recentPerformance = this.performanceHistory.slice(-window);

    if (recentPerformance.length < window) return false;

    switch (criteria.type) {
      case 'threshold':
        const avgPerformance =
          recentPerformance.reduce((a, b) => a + b, 0) / window;
        return avgPerformance >= criteria.value;

      case 'improvement':
        if (recentPerformance.length < 2) return false;
        const improvement =
          recentPerformance[recentPerformance.length - 1] -
          recentPerformance[0];
        return improvement >= criteria.value;

      case 'convergence':
        const variance = this.calculateVariance(recentPerformance);
        return variance <= criteria.value;

      default:
        return false;
    }
  }

  private progressToNextStage(): boolean {
    if (this.currentStage < this.curriculum.stages.length - 1) {
      this.currentStage++;
      this.stageProgress.clear();
      this.performanceHistory = [];

      runtimeLogger.info(
        `ðŸ“ˆ Progressed to curriculum stage ${this.currentStage + 1}: ` +
          `${this.getCurrentStage().name}`
      );

      return true;
    }

    return false;
  }

  private calculateVariance(values: number[]): number {
    const mean = values.reduce((a, b) => a + b, 0) / values.length;
    return (
      values.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) /
      values.length
    );
  }

  getProgress(): {
    currentStage: number;
    totalStages: number;
    stageProgress: number;
    overallProgress: number;
  } {
    const totalStages = this.curriculum.stages.length;
    const stageProgress = this.calculateStageProgress();
    const overallProgress = (this.currentStage + stageProgress) / totalStages;

    return {
      currentStage: this.currentStage,
      totalStages,
      stageProgress,
      overallProgress,
    };
  }

  private calculateStageProgress(): number {
    const stage = this.getCurrentStage();
    const taskCount = stage.tasks.length;

    if (taskCount === 0) return 0;

    let completedTasks = 0;
    for (const task of stage.tasks) {
      const progress = this.stageProgress.get(task.id) || 0;
      if (progress >= stage.completionCriteria.value) {
        completedTasks++;
      }
    }

    return completedTasks / taskCount;
  }
}

/**
 * RL algorithm implementations
 */
class RLAlgorithmEngine {
  private qTable: Map<string, Map<string, number>> = new Map();
  private policyNetwork: Map<string, number[]> = new Map();
  private valueNetwork: Map<string, number> = new Map();
  private experienceBuffer: Experience[] = [];
  private updateCount: number = 0;

  constructor(
    private algorithm: RLAlgorithm,
    private discountFactor: number
  ) {}

  selectAction(
    state: any,
    availableActions: any[],
    explorationStrategy: ExplorationStrategy
  ): any {
    const epsilon = this.getExplorationRate(explorationStrategy);

    if (Math.random() < epsilon) {
      // Exploration: random action
      return availableActions[
        Math.floor(Math.random() * availableActions.length)
      ];
    }

    // Exploitation: best action according to current policy
    switch (this.algorithm) {
      case RLAlgorithm.DQN:
        return this.selectActionDQN(state, availableActions);

      case RLAlgorithm.PPO:
        return this.selectActionPPO(state, availableActions);

      case RLAlgorithm.A3C:
        return this.selectActionA3C(state, availableActions);

      default:
        return this.selectActionDefault(state, availableActions);
    }
  }

  update(
    experience: Experience,
    rewardModel?: RewardModel,
    rewardShaping?: RewardShaping
  ): { loss: number; valueEstimate: number } {
    this.experienceBuffer.push(experience);
    this.updateCount++;

    // Apply reward shaping if provided
    let shapedReward = experience.reward.value;
    if (rewardShaping) {
      shapedReward = rewardShaping.shapeReward(
        experience.reward.value,
        experience.state,
        experience.action,
        experience.nextState
      );
    }

    // Use reward model if provided
    if (rewardModel) {
      const modelReward = rewardModel.predict(
        experience.state,
        experience.action
      );
      const uncertainty = rewardModel.uncertainty(
        experience.state,
        experience.action
      );

      // Blend model reward with environment reward based on uncertainty
      shapedReward =
        (1 - uncertainty) * modelReward + uncertainty * shapedReward;
    }

    // Update based on algorithm
    switch (this.algorithm) {
      case RLAlgorithm.DQN:
        return this.updateDQN(experience, shapedReward);

      case RLAlgorithm.PPO:
        return this.updatePPO(experience, shapedReward);

      case RLAlgorithm.A3C:
        return this.updateA3C(experience, shapedReward);

      default:
        return this.updateDefault(experience, shapedReward);
    }
  }

  private getExplorationRate(strategy: ExplorationStrategy): number {
    const decayed =
      strategy.initialValue *
      Math.pow(strategy.decayRate, this.updateCount / 1000);

    return Math.max(strategy.minValue, decayed);
  }

  private getStateKey(state: any): string {
    return JSON.stringify(state);
  }

  private selectActionDQN(state: any, actions: any[]): any {
    const stateKey = this.getStateKey(state);

    if (!this.qTable.has(stateKey)) {
      this.qTable.set(stateKey, new Map());
    }

    const qValues = this.qTable.get(stateKey)!;
    let bestAction = actions[0];
    let bestValue = -Infinity;

    for (const action of actions) {
      const actionKey = JSON.stringify(action);
      const value = qValues.get(actionKey) || 0;

      if (value > bestValue) {
        bestValue = value;
        bestAction = action;
      }
    }

    return bestAction;
  }

  private selectActionPPO(state: any, actions: any[]): any {
    const stateKey = this.getStateKey(state);
    const policy = this.policyNetwork.get(stateKey) || [];

    if (policy.length === 0) {
      // Initialize uniform policy
      const uniform = new Array(actions.length).fill(1 / actions.length);
      this.policyNetwork.set(stateKey, uniform);
      return actions[Math.floor(Math.random() * actions.length)];
    }

    // Sample from policy distribution
    const r = Math.random();
    let cumSum = 0;

    for (let i = 0; i < Math.min(policy.length, actions.length); i++) {
      cumSum += policy[i];
      if (r <= cumSum) {
        return actions[i];
      }
    }

    return actions[actions.length - 1];
  }

  private selectActionA3C(state: any, actions: any[]): any {
    // Similar to PPO but with actor-critic architecture
    return this.selectActionPPO(state, actions);
  }

  private selectActionDefault(state: any, actions: any[]): any {
    return this.selectActionDQN(state, actions);
  }

  private updateDQN(
    experience: Experience,
    reward: number
  ): { loss: number; valueEstimate: number } {
    const stateKey = this.getStateKey(experience.state);
    const nextStateKey = this.getStateKey(experience.nextState);
    const actionKey = JSON.stringify(experience.action);

    // Initialize Q-tables if needed
    if (!this.qTable.has(stateKey)) {
      this.qTable.set(stateKey, new Map());
    }
    if (!this.qTable.has(nextStateKey)) {
      this.qTable.set(nextStateKey, new Map());
    }

    const qCurrent = this.qTable.get(stateKey)!.get(actionKey) || 0;

    // Get max Q-value for next state
    let maxNextQ = 0;
    const nextQValues = this.qTable.get(nextStateKey)!;
    for (const q of nextQValues.values()) {
      maxNextQ = Math.max(maxNextQ, q);
    }

    // Q-learning update
    const target =
      reward + this.discountFactor * maxNextQ * (experience.done ? 0 : 1);
    const loss = Math.pow(target - qCurrent, 2);
    const learningRate = 0.1;

    const newQ = qCurrent + learningRate * (target - qCurrent);
    this.qTable.get(stateKey)!.set(actionKey, newQ);

    return { loss, valueEstimate: newQ };
  }

  private updatePPO(
    experience: Experience,
    reward: number
  ): { loss: number; valueEstimate: number } {
    const stateKey = this.getStateKey(experience.state);
    const nextStateKey = this.getStateKey(experience.nextState);

    // Update value network
    const currentValue = this.valueNetwork.get(stateKey) || 0;
    const nextValue = this.valueNetwork.get(nextStateKey) || 0;

    const target =
      reward + this.discountFactor * nextValue * (experience.done ? 0 : 1);
    const advantage = target - currentValue;

    // Update value
    const valueLearningRate = 0.01;
    this.valueNetwork.set(
      stateKey,
      currentValue + valueLearningRate * advantage
    );

    // Update policy (simplified)
    const policy = this.policyNetwork.get(stateKey) || [];
    if (policy.length > 0) {
      // Increase probability of taken action if advantage is positive
      const actionIndex = 0; // Simplified - would need action mapping
      const clipRange = 0.2;

      for (let i = 0; i < policy.length; i++) {
        if (i === actionIndex) {
          const ratio = 1 + clipRange * Math.sign(advantage);
          policy[i] = Math.min(1, Math.max(0, policy[i] * ratio));
        }
      }

      // Normalize
      const sum = policy.reduce((a, b) => a + b, 0);
      if (sum > 0) {
        for (let i = 0; i < policy.length; i++) {
          policy[i] /= sum;
        }
      }

      this.policyNetwork.set(stateKey, policy);
    }

    return { loss: Math.abs(advantage), valueEstimate: target };
  }

  private updateA3C(
    experience: Experience,
    reward: number
  ): { loss: number; valueEstimate: number } {
    // Similar to PPO for this simplified implementation
    return this.updatePPO(experience, reward);
  }

  private updateDefault(
    experience: Experience,
    reward: number
  ): { loss: number; valueEstimate: number } {
    return this.updateDQN(experience, reward);
  }

  getStatistics(): {
    statesExplored: number;
    averageQValue: number;
    policyEntropy: number;
  } {
    const statesExplored = this.qTable.size + this.policyNetwork.size;

    // Calculate average Q-value
    let totalQ = 0;
    let qCount = 0;

    for (const stateQ of this.qTable.values()) {
      for (const q of stateQ.values()) {
        totalQ += q;
        qCount++;
      }
    }

    const averageQValue = qCount > 0 ? totalQ / qCount : 0;

    // Calculate policy entropy
    let totalEntropy = 0;
    let policyCount = 0;

    for (const policy of this.policyNetwork.values()) {
      let entropy = 0;
      for (const p of policy) {
        if (p > 0) {
          entropy -= p * Math.log(p);
        }
      }
      totalEntropy += entropy;
      policyCount++;
    }

    const policyEntropy = policyCount > 0 ? totalEntropy / policyCount : 0;

    return { statesExplored, averageQValue, policyEntropy };
  }
}

/**
 * Reinforcement Learning Module
 */
export class ReinforcementLearningModule extends BaseLearningModule {
  // config inherited from base class as protected

  // Getter for typed config
  private get typedConfig(): RLConfig {
    return this.config as RLConfig;
  }
  private algorithmEngine!: RLAlgorithmEngine;
  private rewardModel?: HumanFeedbackRewardModel;
  private curriculumManager?: CurriculumManager;
  private episodeCount: number = 0;
  private totalReward: number = 0;
  private episodeRewards: number[] = [];
  private multiAgentCoordinator?: MultiAgentCoordinator;

  constructor() {
    super('reinforcement-learning', LearningParadigm.REINFORCEMENT_LEARNING);
  }

  protected async onInitialize(config: LearningConfig): Promise<void> {
    this.config = config;

    // Initialize algorithm engine
    this.algorithmEngine = new RLAlgorithmEngine(
      this.typedConfig.algorithm,
      this.typedConfig.discountFactor
    );

    // Initialize RLHF if needed
    if (
      this.typedConfig.algorithm === RLAlgorithm.PPO ||
      this.typedConfig.algorithm === RLAlgorithm.DQN
    ) {
      this.rewardModel = new HumanFeedbackRewardModel();
    }

    // Initialize curriculum if provided
    if (this.typedConfig.curriculum) {
      this.curriculumManager = new CurriculumManager(
        this.typedConfig.curriculum
      );
    }

    // Initialize multi-agent coordinator if needed
    if (this.typedConfig.multiAgent) {
      this.multiAgentCoordinator = new MultiAgentCoordinator(
        this.typedConfig.multiAgent
      );
    }

    this.state.parameters = {
      algorithm: this.typedConfig.algorithm,
      discountFactor: this.typedConfig.discountFactor,
      explorationRate: this.typedConfig.explorationStrategy.initialValue,
    };

    runtimeLogger.info(
      `ðŸŽ® Reinforcement Learning module initialized with ${this.typedConfig.algorithm}`
    );
  }

  protected async onLearn(
    experience: LearningExperience
  ): Promise<LearningUpdate> {
    const changes: ParameterChanges = { updated: {} };

    // Convert to RL experience format
    const rlExperience = this.convertToRLExperience(experience);

    if (rlExperience) {
      // Update with algorithm
      const { loss, valueEstimate } = this.algorithmEngine.update(
        rlExperience,
        this.rewardModel,
        this.typedConfig.rewardShaping
      );

      // Track episode progress
      this.totalReward += rlExperience.reward.value;

      if (rlExperience.done) {
        this.episodeCount++;
        this.episodeRewards.push(this.totalReward);

        // Update curriculum if applicable
        if (this.curriculumManager) {
          const progressed = this.curriculumManager.updateProgress(
            experience.context?.task || 'default',
            this.totalReward
          );

          if (progressed) {
            changes.newCapabilities = ['curriculum_stage_completed'];
          }
        }

        this.totalReward = 0;
      }

      // Update parameters
      const stats = this.algorithmEngine.getStatistics();
      changes.updated.statesExplored = stats.statesExplored;
      changes.updated.averageQValue = stats.averageQValue;
      changes.updated.episodeCount = this.episodeCount;
    }

    // Handle human feedback if available
    if (experience.feedback?.humanFeedback && this.rewardModel) {
      this.rewardModel.update(experience.feedback.humanFeedback);
      changes.updated.humanFeedbackCount =
        ((this.state.parameters.humanFeedbackCount as number) || 0) + 1;
    }

    return {
      paradigm: this.paradigm,
      updateType: UpdateType.GRADIENT,
      metrics: this.calculateMetrics(),
      changes,
      newCapabilities: changes.newCapabilities,
    };
  }

  protected async onAdapt(
    context: AdaptationContext
  ): Promise<AdaptationResult> {
    // Adapt to new environment or task
    if (context.task.name === 'environment_change') {
      return this.adaptToEnvironment(context);
    }

    if (context.task.name === 'reward_function_update') {
      return this.adaptRewardFunction(context);
    }

    // Default: continue with current policy
    return {
      adapted: true,
      performance: this.calculateCurrentPerformance(),
      adaptationTime: 0,
      strategy: 'policy_transfer',
      confidence: 0.7,
    };
  }

  protected async onEvaluate(testData: TestData): Promise<LearningMetrics> {
    let totalReward = 0;
    let episodeCount = 0;
    let successRate = 0;

    // Run evaluation episodes
    for (const example of testData.examples) {
      const episodeReward = await this.runEvaluationEpisode(example);
      totalReward += episodeReward;
      episodeCount++;

      if (episodeReward > 0) {
        successRate++;
      }
    }

    const stats = this.algorithmEngine.getStatistics();

    return {
      averageReward: totalReward / Math.max(1, episodeCount),
      successRate: successRate / Math.max(1, episodeCount),
      statesExplored: stats.statesExplored,
      policyEntropy: stats.policyEntropy,
      curriculumProgress:
        this.curriculumManager?.getProgress().overallProgress || 0,
    };
  }

  protected onReset(): void {
    this.algorithmEngine = new RLAlgorithmEngine(
      this.typedConfig.algorithm,
      this.typedConfig.discountFactor
    );

    this.episodeCount = 0;
    this.totalReward = 0;
    this.episodeRewards = [];

    if (this.rewardModel) {
      this.rewardModel = new HumanFeedbackRewardModel();
    }

    if (this.curriculumManager) {
      this.curriculumManager = new CurriculumManager(
        this.typedConfig.curriculum!
      );
    }
  }

  /**
   * Convert learning experience to RL experience
   */
  private convertToRLExperience(
    experience: LearningExperience
  ): Experience | null {
    if (!experience.feedback?.environmentFeedback) {
      return null;
    }

    const envFeedback = experience.feedback.environmentFeedback;

    return {
      id: experience.id,
      agentId: experience.agentId,
      state: experience.input.state || experience.input,
      action: experience.output.action || {
        type: 'unknown',
        parameters: {},
      },
      reward: {
        id: `reward_${experience.id}`,
        type: 'environment',
        value: envFeedback.reward,
        source: 'environment',
        context: envFeedback.info || {},
        timestamp: experience.timestamp,
        agentId: experience.agentId,
      },
      nextState: envFeedback.stateChange || experience.input.state,
      done: envFeedback.done,
      timestamp: experience.timestamp,
      importance: experience.importance,
      tags: experience.tags || [],
    };
  }

  /**
   * Calculate current metrics
   */
  private calculateMetrics(): LearningMetrics {
    const recentRewards = this.episodeRewards.slice(-100);
    const averageReward =
      recentRewards.length > 0
        ? recentRewards.reduce((a, b) => a + b, 0) / recentRewards.length
        : 0;

    const stats = this.algorithmEngine.getStatistics();

    return {
      episodeCount: this.episodeCount,
      averageReward,
      statesExplored: stats.statesExplored,
      averageQValue: stats.averageQValue,
      policyEntropy: stats.policyEntropy,
      explorationRate:
        this.typedConfig.explorationStrategy.initialValue *
        Math.pow(
          this.typedConfig.explorationStrategy.decayRate,
          this.episodeCount / 100
        ),
    };
  }

  /**
   * Calculate current performance
   */
  private calculateCurrentPerformance(): number {
    if (this.episodeRewards.length === 0) return 0;

    const recentRewards = this.episodeRewards.slice(-10);
    const avgReward =
      recentRewards.reduce((a, b) => a + b, 0) / recentRewards.length;

    // Normalize to 0-1 range (assuming max reward of 100)
    return Math.min(1, Math.max(0, avgReward / 100));
  }

  /**
   * Adapt to new environment
   */
  private async adaptToEnvironment(
    context: AdaptationContext
  ): Promise<AdaptationResult> {
    // Transfer learning: retain value estimates but reset policy exploration
    const currentExploration =
      this.typedConfig.explorationStrategy.initialValue;
    this.typedConfig.explorationStrategy.initialValue = Math.min(
      0.5,
      currentExploration * 2
    );

    // Reset episode tracking for new environment
    this.episodeCount = 0;
    this.episodeRewards = [];

    return {
      adapted: true,
      performance: 0.5, // Baseline performance in new environment
      adaptationTime: 0,
      strategy: 'exploration_reset',
      confidence: 0.6,
    };
  }

  /**
   * Adapt reward function
   */
  private async adaptRewardFunction(
    context: AdaptationContext
  ): Promise<AdaptationResult> {
    if (!this.rewardModel) {
      this.rewardModel = new HumanFeedbackRewardModel();
    }

    // Extract human feedback from context
    for (const example of context.task.supportSet) {
      if (example.output.humanFeedback) {
        this.rewardModel.update(example.output.humanFeedback);
      }
    }

    return {
      adapted: true,
      performance: 0.8,
      adaptationTime: 0,
      strategy: 'reward_modeling',
      confidence: 0.75,
    };
  }

  /**
   * Run evaluation episode
   */
  private async runEvaluationEpisode(example: any): Promise<number> {
    // Simulate episode execution
    let state = example.input.state || example.input;
    let totalReward = 0;
    let done = false;
    let steps = 0;
    const maxSteps = 100;

    while (!done && steps < maxSteps) {
      // Select action
      const action = this.algorithmEngine.selectAction(
        state,
        this.getAvailableActions(state),
        this.typedConfig.explorationStrategy
      );

      // Simulate environment step
      const { nextState, reward, isDone } = this.simulateEnvironmentStep(
        state,
        action
      );

      totalReward += reward;
      state = nextState;
      done = isDone;
      steps++;
    }

    return totalReward;
  }

  /**
   * Get available actions for state
   */
  private getAvailableActions(state: any): any[] {
    // Default action space
    return [
      { type: 'move', direction: 'up' },
      { type: 'move', direction: 'down' },
      { type: 'move', direction: 'left' },
      { type: 'move', direction: 'right' },
      { type: 'stay' },
    ];
  }

  /**
   * Simulate environment step
   */
  private simulateEnvironmentStep(
    state: any,
    action: any
  ): {
    nextState: any;
    reward: number;
    isDone: boolean;
  } {
    // Simple grid world simulation
    const nextState = { ...state };
    let reward = -0.1; // Step penalty

    if (action.type === 'move') {
      // Update position based on action
      switch (action.direction) {
        case 'up':
          nextState.y = (nextState.y || 0) + 1;
          break;
        case 'down':
          nextState.y = (nextState.y || 0) - 1;
          break;
        case 'left':
          nextState.x = (nextState.x || 0) - 1;
          break;
        case 'right':
          nextState.x = (nextState.x || 0) + 1;
          break;
      }
    }

    // Check if reached goal
    const goalX = 5,
      goalY = 5;
    if (nextState.x === goalX && nextState.y === goalY) {
      reward = 10;
      return { nextState, reward, isDone: true };
    }

    // Check bounds
    if (Math.abs(nextState.x || 0) > 10 || Math.abs(nextState.y || 0) > 10) {
      reward = -1;
      return { nextState: state, reward, isDone: true };
    }

    return { nextState, reward, isDone: false };
  }

  /**
   * Select action for external use
   */
  async selectAction(state: any, availableActions?: any[]): Promise<any> {
    const actions = availableActions || this.getAvailableActions(state);
    return this.algorithmEngine.selectAction(
      state,
      actions,
      this.typedConfig.explorationStrategy
    );
  }

  /**
   * Process human feedback
   */
  async processHumanFeedback(feedback: HumanFeedback): Promise<void> {
    if (this.rewardModel) {
      this.rewardModel.update(feedback);

      this.emit('humanFeedbackProcessed', {
        feedbackId: Date.now(),
        uncertainty: this.rewardModel.uncertainty(
          feedback.stateActionPair[0],
          feedback.stateActionPair[1]
        ),
      });
    }
  }
}

/**
 * Multi-agent coordinator
 */
class MultiAgentCoordinator {
  private agentPolicies: Map<string, any> = new Map();
  private communicationBuffer: Map<string, any[]> = new Map();
  private config: MultiAgentConfig;

  constructor(config: MultiAgentConfig) {
    this.config = config;
  }

  coordinate(agentId: string, state: any, action: any): any {
    // Store agent action
    this.agentPolicies.set(agentId, { state, action });

    // Handle communication
    if (this.config.communicationProtocol !== 'none') {
      this.handleCommunication(agentId, { state, action });
    }

    // Apply coordination strategy
    switch (this.config.coordinationStrategy) {
      case 'centralized':
        return this.centralizedCoordination(agentId, state, action);

      case 'decentralized':
        return action; // Agents act independently

      case 'hierarchical':
        return this.hierarchicalCoordination(agentId, state, action);

      default:
        return action;
    }
  }

  private handleCommunication(senderId: string, message: any): void {
    switch (this.config.communicationProtocol) {
      case 'broadcast':
        // Send to all agents
        for (let i = 0; i < this.config.numAgents; i++) {
          const agentId = `agent_${i}`;
          if (agentId !== senderId) {
            if (!this.communicationBuffer.has(agentId)) {
              this.communicationBuffer.set(agentId, []);
            }
            this.communicationBuffer.get(agentId)!.push(message);
          }
        }
        break;

      case 'targeted':
        // Send to specific agents based on proximity or relevance
        // Simplified: send to adjacent agents
        const senderIdx = parseInt(senderId.split('_')[1]);
        const targets = [senderIdx - 1, senderIdx + 1].filter(
          (idx) => idx >= 0 && idx < this.config.numAgents
        );

        for (const targetIdx of targets) {
          const targetId = `agent_${targetIdx}`;
          if (!this.communicationBuffer.has(targetId)) {
            this.communicationBuffer.set(targetId, []);
          }
          this.communicationBuffer.get(targetId)!.push(message);
        }
        break;
    }
  }

  private centralizedCoordination(
    agentId: string,
    state: any,
    action: any
  ): any {
    // Modify action based on global policy
    const allActions = Array.from(this.agentPolicies.values());

    if (
      this.config.sharedReward &&
      allActions.length === this.config.numAgents
    ) {
      // Coordinate actions to maximize shared reward
      // Simplified: avoid conflicts
      const otherActions = allActions.filter((a) => a !== action);

      if (this.hasConflict(action, otherActions)) {
        // Modify action to avoid conflict
        return this.resolveConflict(action, otherActions);
      }
    }

    return action;
  }

  private hierarchicalCoordination(
    agentId: string,
    state: any,
    action: any
  ): any {
    // Leader-follower coordination
    const agentIdx = parseInt(agentId.split('_')[1]);

    if (agentIdx === 0) {
      // Leader: act independently
      return action;
    } else {
      // Follower: consider leader's action
      const leaderPolicy = this.agentPolicies.get('agent_0');
      if (leaderPolicy) {
        // Follow leader with some exploration
        if (Math.random() < 0.8) {
          return leaderPolicy.action;
        }
      }

      return action;
    }
  }

  private hasConflict(action: any, otherActions: any[]): boolean {
    // Check for action conflicts (e.g., multiple agents trying to occupy same position)
    for (const other of otherActions) {
      if (
        action.type === other.action.type &&
        action.target === other.action.target
      ) {
        return true;
      }
    }

    return false;
  }

  private resolveConflict(action: any, otherActions: any[]): any {
    // Simple conflict resolution: choose alternative action
    const alternatives = this.getAlternativeActions(action);

    for (const alt of alternatives) {
      if (!this.hasConflict(alt, otherActions)) {
        return alt;
      }
    }

    // If no alternative, wait
    return { type: 'wait' };
  }

  private getAlternativeActions(action: any): any[] {
    // Generate alternative actions
    if (action.type === 'move') {
      const directions = ['up', 'down', 'left', 'right'];
      return directions
        .filter((d) => d !== action.direction)
        .map((d) => ({ type: 'move', direction: d }));
    }

    return [{ type: 'wait' }];
  }

  getMessages(agentId: string): any[] {
    const messages = this.communicationBuffer.get(agentId) || [];
    this.communicationBuffer.set(agentId, []); // Clear after reading
    return messages;
  }
}

/**
 * Factory function for creating RL module
 */
export async function createReinforcementLearningModule(
  config: RLConfig
): Promise<ReinforcementLearningModule> {
  const module = new ReinforcementLearningModule();
  await module.initialize(config);
  return module;
}
