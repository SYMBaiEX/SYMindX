---
description: OPTIMIZE performance when implementing caching, memory, or compute-intensive features using 2025 edge computing patterns
globs: **/*.ts, **/*.js, **/workers/**, **/edge/**
---

# Performance Optimization Patterns 2025

## 2025 Performance Architecture Overview

SYMindX implements cutting-edge performance optimization strategies leveraging 2025 technologies including edge computing, WebAssembly, service workers, and advanced caching patterns for sub-100ms AI responses and global scalability.

### Core Performance Principles

**âš¡ Response Time Optimization**

- Sub-200ms agent response times for chat interactions
- Parallel processing for AI portal requests
- Intelligent caching across all system layers

**ðŸš€ Scalability Design**

- Horizontal scaling for agent instances
- Load balancing across AI providers
- Resource pooling and connection management

**ðŸ“Š Resource Efficiency**

- Memory-efficient vector operations
- CPU optimization for embedding calculations
- I/O optimization for database operations

## AI Portal Performance

### Provider Selection Optimization

```typescript
interface PerformanceMetrics {
  responseTime: number;
  tokenThroughput: number;
  errorRate: number;
  costPerToken: number;
  successRate: number;
}

class PerformanceOptimizedPortalSelector {
  private metrics: Map<string, PerformanceMetrics> = new Map();
  private loadBalancer: LoadBalancer;
  
  async selectOptimalProvider(request: GenerationRequest): Promise<string> {
    const candidates = this.getAvailableProviders(request);
    
    // Real-time performance scoring
    const scores = await Promise.all(
      candidates.map(async (provider) => {
        const metrics = await this.getRealtimeMetrics(provider);
        const score = this.calculatePerformanceScore(metrics, request);
        return { provider, score, metrics };
      })
    );
    
    // Sort by performance score (higher is better)
    scores.sort((a, b) => b.score - a.score);
    
    // Select provider with circuit breaker protection
    for (const candidate of scores) {
      if (await this.circuitBreaker.isAvailable(candidate.provider)) {
        return candidate.provider;
      }
    }
    
    throw new Error('No available AI providers');
  }
  
  private calculatePerformanceScore(
    metrics: PerformanceMetrics,
    request: GenerationRequest
  ): number {
    const weights = {
      responseTime: 0.4,
      successRate: 0.3,
      tokenThroughput: 0.2,
      costEfficiency: 0.1
    };
    
    // Normalize metrics to 0-1 scale
    const normalizedResponseTime = Math.max(0, 1 - (metrics.responseTime / 5000)); // 5s max
    const normalizedThroughput = Math.min(1, metrics.tokenThroughput / 1000); // 1000 tokens/s max
    const normalizedCost = Math.max(0, 1 - (metrics.costPerToken / 0.01)); // $0.01/token max
    
    return (
      weights.responseTime * normalizedResponseTime +
      weights.successRate * metrics.successRate +
      weights.tokenThroughput * normalizedThroughput +
      weights.costEfficiency * normalizedCost
    );
  }
}
```

### Request Batching and Streaming

```typescript
class OptimizedRequestProcessor {
  private batchProcessor: BatchProcessor;
  private streamingManager: StreamingManager;
  
  async processRequest(request: GenerationRequest): Promise<GenerationResponse> {
    // Determine optimal processing strategy
    if (this.shouldBatch(request)) {
      return this.batchProcessor.addToBatch(request);
    }
    
    if (this.shouldStream(request)) {
      return this.streamingManager.processStreaming(request);
    }
    
    return this.processImmediate(request);
  }
  
  private shouldBatch(request: GenerationRequest): boolean {
    // Batch non-urgent requests for efficiency
    return (
      !request.urgent &&
      request.maxTokens < 500 &&
      this.batchProcessor.hasCapacity()
    );
  }
  
  private shouldStream(request: GenerationRequest): boolean {
    // Stream for real-time interactions
    return (
      request.stream === true ||
      request.maxTokens > 1000 ||
      request.conversationType === 'realtime'
    );
  }
}

class BatchProcessor {
  private batch: GenerationRequest[] = [];
  private batchSize = 10;
  private batchTimeout = 100; // ms
  private processingPromises: Map<string, Promise<GenerationResponse>> = new Map();
  
  async addToBatch(request: GenerationRequest): Promise<GenerationResponse> {
    const requestId = this.generateRequestId();
    this.batch.push({ ...request, id: requestId });
    
    // Create promise for this specific request
    const promise = new Promise<GenerationResponse>((resolve, reject) => {
      this.processingPromises.set(requestId, { resolve, reject });
    });
    
    // Trigger batch processing if needed
    if (this.batch.length >= this.batchSize) {
      this.processBatch();
    } else if (this.batch.length === 1) {
      // Start timeout for first request in batch
      setTimeout(() => this.processBatch(), this.batchTimeout);
    }
    
    return promise;
  }
  
  private async processBatch(): Promise<void> {
    if (this.batch.length === 0) return;
    
    const currentBatch = [...this.batch];
    this.batch = [];
    
    try {
      // Process batch with optimal provider
      const responses = await this.portalManager.processBatch(currentBatch);
      
      // Resolve individual promises
      responses.forEach((response, index) => {
        const request = currentBatch[index];
        const promise = this.processingPromises.get(request.id);
        if (promise) {
          promise.resolve(response);
          this.processingPromises.delete(request.id);
        }
      });
    } catch (error) {
      // Reject all promises in batch
      currentBatch.forEach(request => {
        const promise = this.processingPromises.get(request.id);
        if (promise) {
          promise.reject(error);
          this.processingPromises.delete(request.id);
        }
      });
    }
  }
}
```

## Memory and Vector Optimization

### Efficient Vector Operations

```typescript
class OptimizedVectorSearch {
  private vectorCache: LRUCache<string, Float32Array>;
  private indexManager: VectorIndexManager;
  
  constructor(config: VectorConfig) {
    this.vectorCache = new LRUCache({
      max: config.cacheSize || 10000,
      ttl: config.cacheTTL || 3600000 // 1 hour
    });
    this.indexManager = new VectorIndexManager(config);
  }
  
  async searchSimilar(
    query: string,
    threshold: number = 0.7,
    limit: number = 10
  ): Promise<SimilarityResult[]> {
    // Check cache for query embedding
    const cacheKey = this.hashQuery(query);
    let queryEmbedding = this.vectorCache.get(cacheKey);
    
    if (!queryEmbedding) {
      // Generate embedding with optimized batch processing
      queryEmbedding = await this.generateOptimizedEmbedding(query);
      this.vectorCache.set(cacheKey, queryEmbedding);
    }
    
    // Use optimized index for similarity search
    return this.indexManager.search(queryEmbedding, threshold, limit);
  }
  
  private async generateOptimizedEmbedding(text: string): Promise<Float32Array> {
    // Optimize embedding generation with caching and batching
    const embedding = await this.embeddingProvider.generateEmbedding(text);
    
    // Convert to Float32Array for better memory efficiency
    return new Float32Array(embedding);
  }
}

class VectorIndexManager {
  private index: HNSWIndex | IVFIndex;
  private indexType: 'hnsw' | 'ivf';
  
  constructor(config: VectorConfig) {
    this.indexType = config.indexType || 'hnsw';
    this.initializeIndex(config);
  }
  
  async search(
    queryVector: Float32Array,
    threshold: number,
    limit: number
  ): Promise<SimilarityResult[]> {
    const startTime = performance.now();
    
    let results: SimilarityResult[];
    
    if (this.indexType === 'hnsw') {
      // HNSW: Better for high-dimensional vectors
      results = await this.index.searchKNN(queryVector, limit);
    } else {
      // IVF: Better for large datasets
      results = await this.index.searchWithThreshold(queryVector, threshold, limit);
    }
    
    // Filter by threshold and sort by similarity
    const filtered = results
      .filter(r => r.similarity >= threshold)
      .sort((a, b) => b.similarity - a.similarity)
      .slice(0, limit);
    
    const searchTime = performance.now() - startTime;
    this.metricsCollector.recordVectorSearch(searchTime, filtered.length);
    
    return filtered;
  }
}
```

### Memory Pool Management

```typescript
class AgentMemoryPool {
  private pools: Map<string, ObjectPool> = new Map();
  private memoryMonitor: MemoryMonitor;
  
  constructor() {
    this.memoryMonitor = new MemoryMonitor();
    this.initializePools();
  }
  
  async getAgent(agentType: string): Promise<Agent> {
    const pool = this.pools.get(agentType);
    if (!pool) {
      throw new Error(`No pool configured for agent type: ${agentType}`);
    }
    
    // Get agent from pool or create new one
    const agent = await pool.acquire();
    
    // Reset agent state for reuse
    await agent.reset();
    
    return agent;
  }
  
  async releaseAgent(agent: Agent): Promise<void> {
    const pool = this.pools.get(agent.type);
    if (pool) {
      // Clean up agent resources
      await agent.cleanup();
      
      // Return to pool for reuse
      await pool.release(agent);
    }
  }
  
  private initializePools(): void {
    const agentTypes = ['chat', 'analysis', 'coding', 'creative'];
    
    agentTypes.forEach(type => {
      this.pools.set(type, new ObjectPool({
        create: () => this.createAgent(type),
        destroy: (agent) => agent.destroy(),
        validate: (agent) => agent.isHealthy(),
        min: 2,      // Minimum pool size
        max: 10,     // Maximum pool size
        idleTimeout: 300000 // 5 minutes
      }));
    });
  }
}
```

## Database Performance Optimization

### Connection Pool Optimization

```typescript
class OptimizedConnectionPool {
  private pool: Pool;
  private metrics: PoolMetrics;
  private loadBalancer: DatabaseLoadBalancer;
  
  constructor(config: DatabaseConfig) {
    this.pool = new Pool({
      connectionString: config.primaryUrl,
      min: config.minConnections || 5,
      max: config.maxConnections || 25,
      idleTimeoutMillis: config.idleTimeout || 30000,
      connectionTimeoutMillis: config.connectionTimeout || 5000,
      
      // Performance optimizations
      allowExitOnIdle: true,
      application_name: 'symindx-agent-pool'
    });
    
    this.loadBalancer = new DatabaseLoadBalancer(config);
    this.metrics = new PoolMetrics();
    this.setupMonitoring();
  }
  
  async query<T>(sql: string, params?: any[]): Promise<T[]> {
    const startTime = performance.now();
    
    try {
      // Route query to optimal database instance
      const connection = await this.loadBalancer.getConnection(sql);
      const result = await connection.query(sql, params);
      
      const duration = performance.now() - startTime;
      this.metrics.recordQuery(duration, true);
      
      return result.rows;
    } catch (error) {
      const duration = performance.now() - startTime;
      this.metrics.recordQuery(duration, false);
      throw error;
    }
  }
  
  private setupMonitoring(): void {
    // Monitor pool health every 30 seconds
    setInterval(() => {
      const stats = {
        totalConnections: this.pool.totalCount,
        idleConnections: this.pool.idleCount,
        waitingClients: this.pool.waitingCount
      };
      
      this.metrics.recordPoolStats(stats);
      
      // Alert if pool is under pressure
      if (stats.waitingClients > 5) {
        console.warn('Database pool under pressure:', stats);
      }
    }, 30000);
  }
}
```

### Query Optimization Patterns

```typescript
class QueryOptimizer {
  private queryCache: Map<string, CachedQuery> = new Map();
  private preparedStatements: Map<string, PreparedStatement> = new Map();
  
  async optimizeQuery(sql: string, params: any[]): Promise<string> {
    const queryHash = this.hashQuery(sql, params);
    
    // Check if we have a cached optimization
    const cached = this.queryCache.get(queryHash);
    if (cached && !this.isExpired(cached)) {
      return cached.optimizedSql;
    }
    
    // Analyze and optimize query
    const optimized = await this.analyzeAndOptimize(sql, params);
    
    // Cache the optimization
    this.queryCache.set(queryHash, {
      originalSql: sql,
      optimizedSql: optimized,
      createdAt: Date.now(),
      ttl: 3600000 // 1 hour
    });
    
    return optimized;
  }
  
  private async analyzeAndOptimize(sql: string, params: any[]): Promise<string> {
    let optimized = sql;
    
    // Add LIMIT clauses to prevent runaway queries
    if (!sql.toLowerCase().includes('limit') && sql.toLowerCase().includes('select')) {
      optimized += ' LIMIT 1000';
    }
    
    // Optimize JOIN orders based on table sizes
    if (sql.toLowerCase().includes('join')) {
      optimized = await this.optimizeJoinOrder(optimized);
    }
    
    // Add appropriate indexes hints
    optimized = this.addIndexHints(optimized);
    
    return optimized;
  }
  
  async executePreparedStatement(
    name: string, 
    sql: string, 
    params: any[]
  ): Promise<any> {
    if (!this.preparedStatements.has(name)) {
      // Prepare statement for reuse
      const prepared = await this.pool.prepare(name, sql);
      this.preparedStatements.set(name, prepared);
    }
    
    const statement = this.preparedStatements.get(name)!;
    return statement.execute(params);
  }
}
```

## Real-time Performance Monitoring

### Performance Metrics Collection

```typescript
interface PerformanceMetrics {
  responseTime: number;
  throughput: number;
  errorRate: number;
  memoryUsage: number;
  cpuUsage: number;
  activeConnections: number;
}

class PerformanceMonitor {
  private metrics: CircularBuffer<PerformanceMetrics>;
  private alertThresholds: AlertThresholds;
  private collectors: MetricsCollector[];
  
  constructor(config: MonitoringConfig) {
    this.metrics = new CircularBuffer(config.bufferSize || 1000);
    this.alertThresholds = config.alertThresholds;
    this.setupCollectors();
    this.startMonitoring();
  }
  
  private setupCollectors(): void {
    this.collectors = [
      new ResponseTimeCollector(),
      new ThroughputCollector(),
      new MemoryCollector(),
      new DatabaseCollector(),
      new AIPortalCollector()
    ];
  }
  
  private async startMonitoring(): void {
    setInterval(async () => {
      const metrics = await this.collectMetrics();
      this.metrics.push(metrics);
      
      // Check for performance issues
      await this.checkAlerts(metrics);
      
      // Export metrics to external systems
      await this.exportMetrics(metrics);
    }, 5000); // Collect every 5 seconds
  }
  
  private async collectMetrics(): Promise<PerformanceMetrics> {
    const results = await Promise.all(
      this.collectors.map(collector => collector.collect())
    );
    
    return {
      responseTime: results[0].avgResponseTime,
      throughput: results[1].requestsPerSecond,
      errorRate: results[0].errorRate,
      memoryUsage: results[2].memoryUsage,
      cpuUsage: results[2].cpuUsage,
      activeConnections: results[3].activeConnections
    };
  }
  
  private async checkAlerts(metrics: PerformanceMetrics): Promise<void> {
    const alerts: Alert[] = [];
    
    // Response time alerts
    if (metrics.responseTime > this.alertThresholds.maxResponseTime) {
      alerts.push({
        type: 'performance',
        severity: 'warning',
        message: `High response time: ${metrics.responseTime}ms`,
        metric: 'response_time',
        value: metrics.responseTime
      });
    }
    
    // Memory usage alerts
    if (metrics.memoryUsage > this.alertThresholds.maxMemoryUsage) {
      alerts.push({
        type: 'resource',
        severity: 'critical',
        message: `High memory usage: ${metrics.memoryUsage}%`,
        metric: 'memory_usage',
        value: metrics.memoryUsage
      });
    }
    
    // Error rate alerts
    if (metrics.errorRate > this.alertThresholds.maxErrorRate) {
      alerts.push({
        type: 'reliability',
        severity: 'critical',
        message: `High error rate: ${metrics.errorRate}%`,
        metric: 'error_rate',
        value: metrics.errorRate
      });
    }
    
    // Send alerts
    for (const alert of alerts) {
      await this.alertManager.sendAlert(alert);
    }
  }
}
```

### Auto-scaling and Load Balancing

```typescript
class AutoScaler {
  private scalingPolicy: ScalingPolicy;
  private currentScale: number = 1;
  private scaleHistory: ScaleEvent[] = [];
  
  constructor(policy: ScalingPolicy) {
    this.scalingPolicy = policy;
    this.startScalingLoop();
  }
  
  private async startScalingLoop(): void {
    setInterval(async () => {
      const metrics = await this.performanceMonitor.getCurrentMetrics();
      const decision = await this.makeScalingDecision(metrics);
      
      if (decision.action !== 'none') {
        await this.executeScaling(decision);
      }
    }, this.scalingPolicy.evaluationInterval);
  }
  
  private async makeScalingDecision(
    metrics: PerformanceMetrics
  ): Promise<ScalingDecision> {
    const { responseTime, cpuUsage, memoryUsage, throughput } = metrics;
    
    // Scale up conditions
    const shouldScaleUp = (
      responseTime > this.scalingPolicy.scaleUpThresholds.responseTime ||
      cpuUsage > this.scalingPolicy.scaleUpThresholds.cpuUsage ||
      memoryUsage > this.scalingPolicy.scaleUpThresholds.memoryUsage
    );
    
    // Scale down conditions (only if not recently scaled up)
    const shouldScaleDown = (
      this.canScaleDown() &&
      responseTime < this.scalingPolicy.scaleDownThresholds.responseTime &&
      cpuUsage < this.scalingPolicy.scaleDownThresholds.cpuUsage &&
      memoryUsage < this.scalingPolicy.scaleDownThresholds.memoryUsage
    );
    
    if (shouldScaleUp && this.currentScale < this.scalingPolicy.maxInstances) {
      return {
        action: 'scale_up',
        targetScale: Math.min(
          this.currentScale + 1,
          this.scalingPolicy.maxInstances
        ),
        reason: 'High resource utilization detected'
      };
    }
    
    if (shouldScaleDown && this.currentScale > this.scalingPolicy.minInstances) {
      return {
        action: 'scale_down',
        targetScale: Math.max(
          this.currentScale - 1,
          this.scalingPolicy.minInstances
        ),
        reason: 'Low resource utilization detected'
      };
    }
    
    return { action: 'none', targetScale: this.currentScale, reason: 'No scaling needed' };
  }
  
  private async executeScaling(decision: ScalingDecision): Promise<void> {
    console.log(`Scaling ${decision.action} to ${decision.targetScale} instances: ${decision.reason}`);
    
    try {
      if (decision.action === 'scale_up') {
        await this.launchInstances(decision.targetScale - this.currentScale);
      } else {
        await this.terminateInstances(this.currentScale - decision.targetScale);
      }
      
      this.currentScale = decision.targetScale;
      this.recordScaleEvent(decision);
      
    } catch (error) {
      console.error('Scaling operation failed:', error);
      await this.alertManager.sendAlert({
        type: 'scaling',
        severity: 'error',
        message: `Failed to ${decision.action}: ${error.message}`
      });
    }
  }
}
```

## Configuration and Tuning

### Performance Configuration

```typescript
interface PerformanceConfig {
  aiPortals: {
    requestTimeout: number;        // Default: 30000ms
    concurrentRequests: number;    // Default: 10
    retryAttempts: number;         // Default: 3
    circuitBreakerThreshold: number; // Default: 0.5
  };
  
  database: {
    poolSize: {
      min: number;                 // Default: 5
      max: number;                 // Default: 25
    };
    queryTimeout: number;          // Default: 10000ms
    connectionTimeout: number;     // Default: 5000ms
    statementTimeout: number;      // Default: 30000ms
  };
  
  cache: {
    memoryLimit: string;           // Default: "512MB"
    ttl: {
      embeddings: number;          // Default: 3600s
      conversations: number;       // Default: 1800s
      agents: number;              // Default: 300s
    };
    evictionPolicy: 'lru' | 'lfu'; // Default: 'lru'
  };
  
  monitoring: {
    metricsInterval: number;       // Default: 5000ms
    alertThresholds: {
      responseTime: number;        // Default: 2000ms
      errorRate: number;           // Default: 0.05 (5%)
      memoryUsage: number;         // Default: 0.85 (85%)
      cpuUsage: number;            // Default: 0.80 (80%)
    };
  };
  
  scaling: {
    enabled: boolean;              // Default: true
    minInstances: number;          // Default: 1
    maxInstances: number;          // Default: 10
    evaluationInterval: number;    // Default: 60000ms
    cooldownPeriod: number;        // Default: 300000ms
  };
}
```

## Performance Best Practices

**âš¡ Response Time Optimization**

- Use connection pooling for all external services
- Implement intelligent caching at multiple layers
- Optimize AI provider selection based on real-time metrics
- Batch non-urgent requests for efficiency

**ðŸš€ Scalability Patterns**

- Design stateless agents for horizontal scaling
- Use load balancing across multiple AI providers
- Implement circuit breakers for fault tolerance
- Monitor and auto-scale based on performance metrics

**ðŸ“Š Resource Management**

- Pool and reuse expensive objects (embeddings, connections)
- Implement memory-efficient vector operations
- Use prepared statements for frequent queries
- Monitor and optimize garbage collection

**ðŸ”§ Continuous Optimization**

- Profile code regularly to identify bottlenecks
- A/B test different optimization strategies
- Monitor performance trends over time
- Automate performance regression detection
