---
description: APPLY data management best practices when working with memory providers
globs: mind-agents/src/memory/**/*
---

# Data Management and Database Patterns

## Data Architecture Overview

SYMindX implements a multi-provider data storage architecture supporting SQLite, PostgreSQL, Supabase, and Neon with vector embeddings, conversation persistence, and real-time synchronization across multiple agent instances.

### Core Data Principles

**üóÉÔ∏è Provider Abstraction**

- Unified interface across all memory providers
- Hot-swappable database backends
- Provider-specific optimization strategies

**üìä Vector-First Design**

- All text data stored with semantic embeddings
- Hybrid search capabilities (text + semantic)
- Efficient similarity search and clustering

**üîÑ Real-time Synchronization**

- Multi-agent conversation synchronization
- Event-driven data updates
- Conflict resolution for concurrent modifications

## Database Schema Design

### Core Agent Tables

```sql
-- Agent registry and metadata
CREATE TABLE agents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(255) NOT NULL,
  character_id VARCHAR(100) NOT NULL,
  status VARCHAR(50) DEFAULT 'inactive',
  config JSONB NOT NULL,
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  last_active TIMESTAMP WITH TIME ZONE
);

-- Character definitions and personalities
CREATE TABLE characters (
  id VARCHAR(100) PRIMARY KEY,
  name VARCHAR(255) NOT NULL,
  description TEXT,
  personality_traits JSONB NOT NULL,
  behavioral_patterns JSONB DEFAULT '{}',
  voice_settings JSONB DEFAULT '{}',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Platform integrations and sessions
CREATE TABLE platform_sessions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  agent_id UUID REFERENCES agents(id) ON DELETE CASCADE,
  platform VARCHAR(50) NOT NULL,
  platform_user_id VARCHAR(255),
  session_data JSONB NOT NULL,
  is_active BOOLEAN DEFAULT true,
  expires_at TIMESTAMP WITH TIME ZONE,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  
  UNIQUE(agent_id, platform, platform_user_id)
);
```

### Conversation and Memory Tables

```sql
-- Conversation threads
CREATE TABLE conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  agent_id UUID REFERENCES agents(id) ON DELETE CASCADE,
  platform VARCHAR(50) NOT NULL,
  platform_conversation_id VARCHAR(255),
  title VARCHAR(255),
  participants JSONB NOT NULL,
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  
  UNIQUE(agent_id, platform, platform_conversation_id)
);

-- Individual messages with vector embeddings
CREATE TABLE messages (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  conversation_id UUID REFERENCES conversations(id) ON DELETE CASCADE,
  agent_id UUID REFERENCES agents(id) ON DELETE CASCADE,
  sender_id VARCHAR(255) NOT NULL,
  sender_name VARCHAR(255),
  content TEXT NOT NULL,
  content_embedding vector(1536), -- OpenAI embedding dimension
  message_type VARCHAR(50) DEFAULT 'text',
  platform_message_id VARCHAR(255),
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  
  -- Vector similarity search index
  INDEX USING ivfflat (content_embedding vector_cosine_ops) WITH (lists = 100)
);

-- Memory fragments for long-term storage
CREATE TABLE memory_fragments (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  agent_id UUID REFERENCES agents(id) ON DELETE CASCADE,
  content TEXT NOT NULL,
  content_embedding vector(1536),
  fragment_type VARCHAR(50) NOT NULL, -- 'conversation', 'knowledge', 'preference', 'experience'
  importance_score FLOAT DEFAULT 0.5,
  access_count INTEGER DEFAULT 0,
  last_accessed TIMESTAMP WITH TIME ZONE,
  source_conversation_id UUID REFERENCES conversations(id) ON DELETE SET NULL,
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  
  INDEX USING ivfflat (content_embedding vector_cosine_ops) WITH (lists = 100)
);
```

### Emotion and Cognition Data

```sql
-- Emotional state tracking
CREATE TABLE emotion_states (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  agent_id UUID REFERENCES agents(id) ON DELETE CASCADE,
  emotion_type VARCHAR(50) NOT NULL,
  intensity FLOAT NOT NULL CHECK (intensity >= 0 AND intensity <= 1),
  triggers JSONB DEFAULT '[]',
  context JSONB DEFAULT '{}',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  expires_at TIMESTAMP WITH TIME ZONE
);

-- Cognitive state and decision history
CREATE TABLE cognition_events (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  agent_id UUID REFERENCES agents(id) ON DELETE CASCADE,
  event_type VARCHAR(50) NOT NULL, -- 'decision', 'planning', 'reflection'
  cognitive_module VARCHAR(50) NOT NULL,
  input_data JSONB NOT NULL,
  output_data JSONB NOT NULL,
  processing_time_ms INTEGER,
  success BOOLEAN DEFAULT true,
  error_message TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

## Data Provider Implementations

### Base Provider Interface

```typescript
interface DataProvider {
  // Connection management
  connect(): Promise<void>;
  disconnect(): Promise<void>;
  healthCheck(): Promise<boolean>;
  
  // Agent management
  createAgent(agent: AgentData): Promise<Agent>;
  getAgent(agentId: string): Promise<Agent | null>;
  updateAgent(agentId: string, updates: Partial<AgentData>): Promise<Agent>;
  deleteAgent(agentId: string): Promise<void>;
  listAgents(filters?: AgentFilters): Promise<Agent[]>;
  
  // Conversation management
  createConversation(conversation: ConversationData): Promise<Conversation>;
  getConversation(conversationId: string): Promise<Conversation | null>;
  addMessage(message: MessageData): Promise<Message>;
  getMessages(conversationId: string, options?: MessageQueryOptions): Promise<Message[]>;
  
  // Memory operations
  storeMemoryFragment(fragment: MemoryFragmentData): Promise<MemoryFragment>;
  searchMemories(query: string, options?: SearchOptions): Promise<MemoryFragment[]>;
  getRelevantMemories(context: string, limit?: number): Promise<MemoryFragment[]>;
  
  // Vector operations
  generateEmbedding(text: string): Promise<number[]>;
  similaritySearch(embedding: number[], threshold?: number): Promise<SimilarityResult[]>;
  
  // Transactions and consistency
  transaction<T>(callback: (tx: Transaction) => Promise<T>): Promise<T>;
}
```

### PostgreSQL Provider Implementation

```typescript
class PostgreSQLProvider implements DataProvider {
  private pool: Pool;
  private vectorClient: VectorClient;
  
  constructor(config: PostgreSQLConfig) {
    this.pool = new Pool({
      connectionString: config.connectionString,
      max: config.maxConnections || 20,
      idleTimeoutMillis: config.idleTimeout || 30000,
      connectionTimeoutMillis: config.connectionTimeout || 2000,
    });
  }
  
  async addMessage(message: MessageData): Promise<Message> {
    const client = await this.pool.connect();
    
    try {
      await client.query('BEGIN');
      
      // Generate embedding for content
      const embedding = await this.generateEmbedding(message.content);
      
      // Insert message with embedding
      const result = await client.query(`
        INSERT INTO messages (
          conversation_id, agent_id, sender_id, sender_name,
          content, content_embedding, message_type, platform_message_id, metadata
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
        RETURNING *
      `, [
        message.conversationId,
        message.agentId,
        message.senderId,
        message.senderName,
        message.content,
        `[${embedding.join(',')}]`, // PostgreSQL vector format
        message.messageType || 'text',
        message.platformMessageId,
        JSON.stringify(message.metadata || {})
      ]);
      
      // Update conversation timestamp
      await client.query(`
        UPDATE conversations 
        SET updated_at = NOW() 
        WHERE id = $1
      `, [message.conversationId]);
      
      await client.query('COMMIT');
      
      return this.mapRowToMessage(result.rows[0]);
    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }
  
  async searchMemories(query: string, options: SearchOptions = {}): Promise<MemoryFragment[]> {
    const queryEmbedding = await this.generateEmbedding(query);
    const threshold = options.threshold || 0.7;
    const limit = options.limit || 10;
    
    const result = await this.pool.query(`
      SELECT *
      FROM memory_fragments
      WHERE agent_id = $1
        AND content_embedding <=> $2 < $3
      ORDER BY content_embedding <=> $2
      LIMIT $4
    `, [
      options.agentId,
      `[${queryEmbedding.join(',')}]`,
      1 - threshold, // Convert similarity to distance
      limit
    ]);
    
    return result.rows.map(row => this.mapRowToMemoryFragment(row));
  }
}
```

### Supabase Provider Implementation

```typescript
class SupabaseProvider implements DataProvider {
  private client: SupabaseClient;
  private vectorSearch: VectorSearch;
  
  constructor(config: SupabaseConfig) {
    this.client = createClient(config.url, config.anonKey);
    this.vectorSearch = new VectorSearch(config.embeddingProvider);
  }
  
  async addMessage(message: MessageData): Promise<Message> {
    // Generate embedding
    const embedding = await this.generateEmbedding(message.content);
    
    // Insert message with RPC call for vector operations
    const { data, error } = await this.client
      .rpc('insert_message_with_embedding', {
        p_conversation_id: message.conversationId,
        p_agent_id: message.agentId,
        p_sender_id: message.senderId,
        p_sender_name: message.senderName,
        p_content: message.content,
        p_content_embedding: embedding,
        p_message_type: message.messageType || 'text',
        p_platform_message_id: message.platformMessageId,
        p_metadata: message.metadata || {}
      });
    
    if (error) {
      throw new DatabaseError(`Failed to insert message: ${error.message}`);
    }
    
    return data;
  }
  
  async getRelevantMemories(context: string, limit: number = 5): Promise<MemoryFragment[]> {
    const contextEmbedding = await this.generateEmbedding(context);
    
    const { data, error } = await this.client
      .rpc('search_memory_fragments', {
        query_embedding: contextEmbedding,
        similarity_threshold: 0.7,
        match_count: limit
      });
    
    if (error) {
      throw new DatabaseError(`Memory search failed: ${error.message}`);
    }
    
    // Update access counts
    if (data.length > 0) {
      await this.client
        .from('memory_fragments')
        .update({ 
          access_count: data[0].access_count + 1,
          last_accessed: new Date().toISOString()
        })
        .in('id', data.map(f => f.id));
    }
    
    return data;
  }
}
```

## Migration Management

### Schema Migration Framework

```typescript
interface Migration {
  version: string;
  description: string;
  up: (provider: DataProvider) => Promise<void>;
  down: (provider: DataProvider) => Promise<void>;
}

class MigrationManager {
  private migrations: Map<string, Migration> = new Map();
  private provider: DataProvider;
  
  constructor(provider: DataProvider) {
    this.provider = provider;
    this.loadMigrations();
  }
  
  async migrate(targetVersion?: string): Promise<void> {
    const currentVersion = await this.getCurrentVersion();
    const sortedMigrations = this.getSortedMigrations();
    
    for (const migration of sortedMigrations) {
      if (targetVersion && migration.version > targetVersion) {
        break;
      }
      
      if (migration.version > currentVersion) {
        console.log(`Running migration ${migration.version}: ${migration.description}`);
        
        await this.provider.transaction(async (tx) => {
          await migration.up(this.provider);
          await this.recordMigration(migration.version);
        });
        
        console.log(`‚úÖ Migration ${migration.version} completed`);
      }
    }
  }
  
  async rollback(targetVersion: string): Promise<void> {
    const currentVersion = await this.getCurrentVersion();
    const sortedMigrations = this.getSortedMigrations().reverse();
    
    for (const migration of sortedMigrations) {
      if (migration.version <= targetVersion) {
        break;
      }
      
      if (migration.version <= currentVersion) {
        console.log(`Rolling back migration ${migration.version}`);
        
        await this.provider.transaction(async (tx) => {
          await migration.down(this.provider);
          await this.removeMigrationRecord(migration.version);
        });
        
        console.log(`‚Ü©Ô∏è Migration ${migration.version} rolled back`);
      }
    }
  }
}
```

### Example Migrations

```typescript
// Migration: Add emotion intensity tracking
const migration_20250103_001: Migration = {
  version: '20250103_001',
  description: 'Add emotion intensity tracking and triggers',
  
  async up(provider: DataProvider): Promise<void> {
    await provider.query(`
      ALTER TABLE emotion_states 
      ADD COLUMN IF NOT EXISTS intensity FLOAT DEFAULT 0.5 CHECK (intensity >= 0 AND intensity <= 1);
      
      ALTER TABLE emotion_states 
      ADD COLUMN IF NOT EXISTS triggers JSONB DEFAULT '[]';
      
      CREATE INDEX IF NOT EXISTS idx_emotion_states_intensity 
      ON emotion_states(agent_id, emotion_type, intensity DESC);
    `);
  },
  
  async down(provider: DataProvider): Promise<void> {
    await provider.query(`
      DROP INDEX IF EXISTS idx_emotion_states_intensity;
      ALTER TABLE emotion_states DROP COLUMN IF EXISTS triggers;
      ALTER TABLE emotion_states DROP COLUMN IF EXISTS intensity;
    `);
  }
};

// Migration: Add vector similarity search optimization
const migration_20250103_002: Migration = {
  version: '20250103_002',
  description: 'Optimize vector similarity search with better indexing',
  
  async up(provider: DataProvider): Promise<void> {
    await provider.query(`
      -- Drop old index
      DROP INDEX IF EXISTS messages_content_embedding_idx;
      
      -- Create optimized IVFFlat index
      CREATE INDEX messages_content_embedding_ivfflat_idx 
      ON messages USING ivfflat (content_embedding vector_cosine_ops) 
      WITH (lists = 100);
      
      -- Create similar index for memory fragments
      CREATE INDEX memory_fragments_embedding_ivfflat_idx 
      ON memory_fragments USING ivfflat (content_embedding vector_cosine_ops) 
      WITH (lists = 50);
      
      -- Add memory importance-based filtering
      CREATE INDEX memory_fragments_importance_idx 
      ON memory_fragments(agent_id, importance_score DESC);
    `);
  },
  
  async down(provider: DataProvider): Promise<void> {
    await provider.query(`
      DROP INDEX IF EXISTS messages_content_embedding_ivfflat_idx;
      DROP INDEX IF EXISTS memory_fragments_embedding_ivfflat_idx;
      DROP INDEX IF EXISTS memory_fragments_importance_idx;
    `);
  }
};
```

## Data Validation Patterns

### Input Validation

```typescript
import { z } from 'zod';

// Agent configuration validation
const AgentConfigSchema = z.object({
  name: z.string().min(1).max(255),
  characterId: z.string().min(1).max(100),
  status: z.enum(['active', 'inactive', 'disabled']).default('inactive'),
  config: z.object({
    memoryProvider: z.enum(['sqlite', 'postgresql', 'supabase', 'neon']),
    emotionModule: z.string(),
    cognitiveModule: z.string(),
    aiPortal: z.string()
  }),
  metadata: z.record(z.any()).default({})
});

// Message validation
const MessageSchema = z.object({
  conversationId: z.string().uuid(),
  agentId: z.string().uuid(),
  senderId: z.string().min(1).max(255),
  senderName: z.string().min(1).max(255).optional(),
  content: z.string().min(1).max(4000),
  messageType: z.enum(['text', 'image', 'file', 'system']).default('text'),
  platformMessageId: z.string().optional(),
  metadata: z.record(z.any()).default({})
});

// Memory fragment validation
const MemoryFragmentSchema = z.object({
  agentId: z.string().uuid(),
  content: z.string().min(1).max(2000),
  fragmentType: z.enum(['conversation', 'knowledge', 'preference', 'experience']),
  importanceScore: z.number().min(0).max(1).default(0.5),
  sourceConversationId: z.string().uuid().optional(),
  metadata: z.record(z.any()).default({})
});

class DataValidator {
  async validateAgentConfig(data: unknown): Promise<AgentData> {
    return AgentConfigSchema.parseAsync(data);
  }
  
  async validateMessage(data: unknown): Promise<MessageData> {
    const validated = await MessageSchema.parseAsync(data);
    
    // Additional business logic validation
    await this.validateConversationExists(validated.conversationId);
    await this.validateAgentExists(validated.agentId);
    await this.validateContentSafety(validated.content);
    
    return validated;
  }
  
  private async validateContentSafety(content: string): Promise<void> {
    // Check for malicious content
    if (this.containsMaliciousPatterns(content)) {
      throw new ValidationError('Content contains potentially malicious patterns');
    }
    
    // Check content length and encoding
    if (Buffer.byteLength(content, 'utf8') > 10000) {
      throw new ValidationError('Content exceeds maximum byte length');
    }
  }
}
```

## Performance Optimization

### Caching Strategies

```typescript
interface CacheConfig {
  provider: 'redis' | 'memory';
  ttl: number;
  maxSize?: number;
  enableMetrics: boolean;
}

class DataCache {
  private cache: Map<string, CacheEntry>;
  private redis?: RedisClient;
  private metrics: CacheMetrics;
  
  constructor(config: CacheConfig) {
    this.cache = new Map();
    this.metrics = new CacheMetrics();
    
    if (config.provider === 'redis') {
      this.redis = new RedisClient(config);
    }
  }
  
  async get<T>(key: string): Promise<T | null> {
    this.metrics.recordAccess(key);
    
    // Try memory cache first
    const memoryEntry = this.cache.get(key);
    if (memoryEntry && !this.isExpired(memoryEntry)) {
      this.metrics.recordHit('memory');
      return memoryEntry.value;
    }
    
    // Try Redis cache if available
    if (this.redis) {
      const redisValue = await this.redis.get(key);
      if (redisValue) {
        const parsed = JSON.parse(redisValue);
        // Populate memory cache
        this.cache.set(key, {
          value: parsed,
          expiresAt: Date.now() + this.config.ttl * 1000
        });
        this.metrics.recordHit('redis');
        return parsed;
      }
    }
    
    this.metrics.recordMiss();
    return null;
  }
  
  async set<T>(key: string, value: T, ttl?: number): Promise<void> {
    const expiresAt = Date.now() + (ttl || this.config.ttl) * 1000;
    
    // Store in memory cache
    this.cache.set(key, { value, expiresAt });
    
    // Store in Redis if available
    if (this.redis) {
      await this.redis.setex(key, ttl || this.config.ttl, JSON.stringify(value));
    }
    
    this.metrics.recordSet(key);
  }
}
```

### Query Optimization

```typescript
class QueryOptimizer {
  // Optimize conversation queries with proper indexing
  async getConversationMessages(
    conversationId: string, 
    options: MessageQueryOptions
  ): Promise<Message[]> {
    const { limit = 50, offset = 0, since, until } = options;
    
    // Build optimized query with selective fields
    let query = `
      SELECT id, sender_id, sender_name, content, message_type, created_at
      FROM messages 
      WHERE conversation_id = $1
    `;
    
    const params: any[] = [conversationId];
    let paramIndex = 2;
    
    // Add time-based filtering
    if (since) {
      query += ` AND created_at >= $${paramIndex}`;
      params.push(since);
      paramIndex++;
    }
    
    if (until) {
      query += ` AND created_at <= $${paramIndex}`;
      params.push(until);
      paramIndex++;
    }
    
    // Order and paginate
    query += ` ORDER BY created_at DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
    params.push(limit, offset);
    
    return this.provider.query(query, params);
  }
  
  // Batch memory fragment retrieval
  async getRelevantMemoriesBatch(
    agentId: string,
    contexts: string[],
    limit: number = 5
  ): Promise<Map<string, MemoryFragment[]>> {
    const results = new Map<string, MemoryFragment[]>();
    
    // Generate embeddings in batch for efficiency
    const embeddings = await this.batchGenerateEmbeddings(contexts);
    
    // Single query with multiple similarity searches
    const query = `
      WITH context_searches AS (
        SELECT unnest($2::vector[]) as query_embedding,
               unnest($3::text[]) as context_key
      )
      SELECT 
        cs.context_key,
        mf.*,
        (mf.content_embedding <=> cs.query_embedding) as distance
      FROM context_searches cs
      CROSS JOIN LATERAL (
        SELECT * FROM memory_fragments mf
        WHERE mf.agent_id = $1
          AND mf.content_embedding <=> cs.query_embedding < 0.3
        ORDER BY mf.content_embedding <=> cs.query_embedding
        LIMIT $4
      ) mf
    `;
    
    const rows = await this.provider.query(query, [
      agentId,
      embeddings.map(e => `[${e.join(',')}]`),
      contexts,
      limit
    ]);
    
    // Group results by context
    for (const row of rows) {
      const contextKey = row.context_key;
      if (!results.has(contextKey)) {
        results.set(contextKey, []);
      }
      results.get(contextKey)!.push(this.mapRowToMemoryFragment(row));
    }
    
    return results;
  }
}
```

## Configuration Management

### Data Provider Configuration

```typescript
interface DataProviderConfig {
  provider: 'sqlite' | 'postgresql' | 'supabase' | 'neon';
  connection: {
    url?: string;
    host?: string;
    port?: number;
    database?: string;
    username?: string;
    password?: string;
  };
  pool: {
    min: number;           // Default: 2
    max: number;           // Default: 20
    idleTimeout: number;   // Default: 30000ms
    acquireTimeout: number; // Default: 60000ms
  };
  cache: {
    enabled: boolean;      // Default: true
    provider: 'redis' | 'memory'; // Default: 'memory'
    ttl: number;          // Default: 3600s
  };
  vector: {
    dimensions: number;    // Default: 1536 (OpenAI)
    provider: 'openai' | 'anthropic' | 'local';
    indexType: 'ivfflat' | 'hnsw'; // Default: 'ivfflat'
  };
  monitoring: {
    enableMetrics: boolean; // Default: true
    slowQueryThreshold: number; // Default: 1000ms
    enableQueryLogging: boolean; // Default: false
  };
}
```

## Data Integrity and Consistency

**üîí ACID Compliance**

- All multi-table operations use transactions
- Referential integrity enforced via foreign keys
- Optimistic locking for concurrent updates

**üìä Vector Consistency**

- Embedding generation retry logic
- Vector index maintenance automation
- Similarity threshold validation

**üîÑ Cross-Provider Synchronization**

- Change data capture for real-time sync
- Conflict resolution strategies
- Backup and restore procedures

**üìà Performance Monitoring**

- Query performance tracking
- Cache hit rate monitoring
- Vector search latency metrics
- Data growth trend analysis

## Related Rules and Integration

### Foundation Requirements
- @001-symindx-workspace.mdc - SYMindX architecture and memory system directory structure
- @003-typescript-standards.mdc - TypeScript development standards for data layer implementation
- @004-architecture-patterns.mdc - Modular design principles and hot-swappable provider patterns
- @.cursor/docs/architecture.md - Detailed memory layer architecture and data flow patterns

### Security and Performance
- @010-security-and-authentication.mdc - Database security, encryption at rest, and connection security
- @012-performance-optimization.mdc - Vector embedding optimization, caching strategies, and query performance
- @015-configuration-management.mdc - Database configuration and connection management
- @.cursor/tools/project-analyzer.md - Database performance analysis and query optimization metrics

### Development Tools and Templates
- @.cursor/tools/code-generator.md - Memory provider templates and database schema generation
- @.cursor/tools/debugging-guide.md - Database debugging strategies and connection troubleshooting
- @.cursor/docs/contributing.md - Development workflow for memory system contributions

### Development and Quality
- @008-testing-and-quality-standards.mdc - Database testing strategies, migration testing, and data integrity validation
- @013-error-handling-logging.mdc - Database error handling patterns and query monitoring
- @009-deployment-and-operations.mdc - Database deployment, backup strategies, and operational monitoring

### Advanced Integration
- @005-ai-integration-patterns.mdc - AI provider integration for embedding generation and vector operations
- @020-mcp-integration.mdc - External database service integration via Model Context Protocol
- @022-workflow-automation.mdc - Automated database maintenance, migration workflows, and optimization tasks

### Component Integration
- @007-extension-system-patterns.mdc - Platform-specific data storage requirements and integration patterns
- @016-documentation-standards.mdc - Database schema documentation and API reference standards

This rule defines the data management foundation that supports all SYMindX components including memory systems, conversation persistence, and vector-based semantic search.
